# Cohort Analysis Pipeline â€“ BigQuery â†’ Fivetran â†’ Databricks

## ğŸ“Œ Executive Summary

Cohort analysis is a key method in e-commerce analytics to understand how customer behavior evolves over time.  
It helps answer essential business questions:

- **How effectively are we acquiring new customers?**  
- **Do first-time buyers return â€” and how quickly?**  
- **Which monthly cohorts perform better or worse?**  
- **Is retention improving or declining over time?**

This project simulates a real-world analytics pipeline for a fictitious online shop (2024).  
Although the Databricks catalog uses the common term *ETL*, the workflow itself follows a modern **ELT pattern** (data is loaded first via Fivetran and transformed entirely inside Databricks).  
Using cloud-native ELT tools (**BigQuery â†’ Fivetran â†’ Databricks**), the pipeline produces insights into:

- Monthly cohort sizes  
- Retention after 1, 2, and 3 months  
- Depth of repeat purchasing  
- Contribution of new vs. existing customers over time  

The entire workflow follows the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold â†’ Serve Layer) and ends in an **interactive Databricks Lakeview dashboard**.

---

## ğŸš€ Architecture Overview

```text
BigQuery (Source)
        â”‚
        â–¼
Fivetran (Managed Ingestion)
        â”‚
        â–¼
Databricks Lakehouse
 â”œâ”€â”€ Bronze  â€“ Raw Fivetran tables
 â”œâ”€â”€ Silver  â€“ Cleaned facts (orders & customers)
 â”œâ”€â”€ Gold    â€“ Cohort analytics tables
 â””â”€â”€ Serve   â€“ Lakeview Dashboard
```

---

## 1. Data Source â€“ BigQuery

The raw dataset `ecom_orders` contains ~1,000 transactions from a fictitious e-commerce store for the year 2024.  
It serves as the single source of truth for this ELT pipeline.

### âœ” BigQuery Dataset

![BigQuery Source](img/source_bigquery.png)

---

## 2. Ingestion Layer â€“ Fivetran

A **Fivetran BigQuery â†’ Databricks** connector performs automated ingestion every 6 hours.  
Schema changes and updates are handled seamlessly.

### Key properties

- Zero-maintenance ingestion  
- Automated sync on schedule  
- Schema evolution supported  
- Loaded into the Databricks catalog as Delta tables  

### âœ” Fivetran Connector

![Fivetran Connector](img/connector_fivetran.png)

---

## 3. Databricks Lakehouse â€“ Medallion Architecture

All transformations are implemented using Databricks SQL & PySpark notebooks.

---

### ğŸ¥‰ Bronze Layer â€“ Raw Data

Contains the unmodified, synchronized table:

- `bronze_ecom_orders`

---

### ğŸ¥ˆ Silver Layer â€“ Clean Business Facts

Two refined fact tables:

#### `silver_customer_facts`

- First purchase date  
- Cohort month  
- Second purchase date  
- Time-to-second-order (1/2/3 months)

#### `silver_order_facts`

- All orders enriched with:
  - new/existing customer status  
  - standardized dates  
  - order_month  

---

### ğŸ¥‡ Gold Layer â€“ Analytics Marts

Final analytical tables feeding the dashboard:

| Table                     | Description |
|--------------------------|-------------|
| `gold_cohort_sizes`      | Customers per cohort month |
| `gold_cohort_retention`  | 1-, 2-, and 3-month retention |
| `gold_repeat_purchases`  | Share reaching 2+, 3+, 4+ orders |
| `gold_order_distribution`| Monthly orders by new/existing customers |

---

## ğŸ“ Databricks Catalog Structure

All tables are stored inside the Databricks lakehouse catalog.

### âœ” Databricks Catalog

![Databricks Catalog](img/databricks_catalog.png)

---

## 4. Pipeline Orchestration â€“ Databricks Jobs

The full pipeline runs as a Databricks job with:

- Bronze â†’ Silver â†’ Gold â†’ Dashboard dependency chain  
- Triggered by **Table Update** of `ecom_orders`  
- Serverless compute  
- Automated dashboard refresh  
- Email notifications  

### âœ” Successful DAG Run

![Databricks DAG](img/databricks_dag.png)

---

## 5. Serve Layer â€“ Cohort Analysis Dashboard

The final dashboard highlights key e-commerce KPIs:

### ğŸ“Š Metrics

- Monthly order volume (new vs. existing customers)
- Cohort sizes
- Retention after 1/2/3 months
- Repeat purchase depth (2+, 3+, 4+ orders)

### ğŸ” Insights (2024)

- **Acquisition declines** sharply from mid-year  
- **1-month retention drops** after May  
- **Repeat purchase depth weakens**, suggesting fewer loyal heavy users  
- Overall â†’ **activation and retention effectiveness is decreasing**

### âœ” Dashboard Screenshot

![Dashboard](img/cohort_dashboard.png)

---

## 6. Repository Structure

```text
ETL-Project-Cohort-Analysis/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_bronze/
â”‚   â”œâ”€â”€ 02_silver/
â”‚   â”œâ”€â”€ 03_gold/
â”‚   â”œâ”€â”€ 04_views/
â”‚   â””â”€â”€ 90_inspect/
â”‚
â”œâ”€â”€ img/
â”‚   â”œâ”€â”€ source_bigquery.png
â”‚   â”œâ”€â”€ connector_fivetran.png
â”‚   â”œâ”€â”€ databricks_catalog.png
â”‚   â”œâ”€â”€ databricks_dag.png
â”‚   â””â”€â”€ cohort_dashboard.png
â”‚
â”œâ”€â”€ docs/        # Optional additional screenshots
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md
```

---

## 7. Technologies Used

- **Google BigQuery** â€“ source system  
- **Fivetran** â€“ managed ingestion  
- **Databricks Lakehouse** â€“ Delta, SQL, PySpark, Jobs, Lakeview  
- **GitHub** â€“ version control  

---

## 8. How to Reproduce

1. Create BigQuery dataset & load `ecom_orders`  
2. Configure Fivetran BigQuery â†’ Databricks connector  
3. Create Databricks catalog & schema  
4. Run Bronze, Silver, Gold notebooks  
5. Build Lakeview Dashboard  
6. Configure Databricks Job Pipeline  
7. Validate using the inspection notebook  

---

## 9. Next Steps (Realistic Extension Options)

Given the limited sample dataset, meaningful improvements include:

- Add product, channel, or demographic dimensions  
- Introduce dbt for modular modeling & testing  
- Add data quality expectations  
- Publish dashboard externally (Tableau / Power BI)  
- Parameterize the pipeline for multiple datasets  

---

## ğŸ‘¤ Author

Thomas Jortzig â€” ELT Pipeline & Visualization for Cohort Analysis (11/2025)
