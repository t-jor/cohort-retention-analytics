# Cohort Retention Modeling & Customer Behavior Analysis (Ecom Case Study)

**Analyzing customer retention, repeat purchasing, and cohort performance for an e-commerce business**  
*A cloud-native ELT pipeline using BigQuery, Fivetran, and Databricks to model acquisition, retention, and lifecycle purchasing behavior.*

## ğŸ“Œ Executive Summary

Cohort analysis is one of the most effective ways to understand how customer behavior evolves over time â€” particularly in e-commerce, where acquisition and retention directly drive revenue.  
It helps answer essential business questions:

- **How effectively are we acquiring new customers?**  
- **Do first-time buyers return â€” and how quickly?**  
- **Which monthly cohorts perform better or worse?**  
- **Is retention improving or declining over time?**

This project simulates a real-world analytics pipeline for a fictitious online shop (2024).  
While Databricks refers to the Unity Catalog as *ETL*, the actual workflow is modern **ELT** (data is loaded first, then transformed entirely in Databricks).  
Using cloud-native ELT tools (**BigQuery â†’ Fivetran â†’ Databricks**), the pipeline produces insights into:

- Monthly cohort sizes  
- Retention after 1, 2, and 3 months  
- Depth of repeat purchasing  
- Contribution of new vs. existing customers over time  

The entire workflow follows the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold â†’ Serve Layer) and ends in an **interactive Databricks Lakeview dashboard**.

---

## ğŸš€ Architecture Overview (ELT Workflow)

```text
BigQuery (Source)
        â”‚
        â–¼
Fivetran (Managed Connector) â€” Extract & Load
        â”‚
        â–¼
Databricks Lakehouse â€” Transform
 â”œâ”€â”€ Bronze  â€“ Raw ingested tables
 â”œâ”€â”€ Silver  â€“ Clean business facts
 â”œâ”€â”€ Gold    â€“ Analytics-ready tables
 â””â”€â”€ Serve   â€“ Lakeview Dashboard
```

---

## 1. Data Source â€“ BigQuery

The raw dataset `ecom_orders` contains ~1,000 transactions from a fictitious e-commerce store for the year 2024.

### âœ” BigQuery Dataset

![BigQuery Source](img/source_bigquery.png)

---

## 2. Ingestion Layer â€“ Fivetran

A **Fivetran BigQuery â†’ Databricks** connector loads the `ecom_orders` table into the Lakehouse on an automated schedule.

### Key properties

- Fully managed ingestion (API changes, schema drift, retries, rate limits, incremental loading)  
- Automated scheduled sync  
- Data loaded into Databricks Unity Catalog as Delta tables

### âœ” Fivetran Connector

![Fivetran Connector](img/connector_fivetran.png)

---

## 3. Databricks Lakehouse â€“ Medallion Architecture

Transformations follow the **Medallion Architecture** described in the Architecture Overview above.
The section below lists the concrete tables implemented in each layer.

---

### ğŸ¥‰ Bronze Layer â€“ Raw & Cleaned Source

| Table                | Description |
|----------------------|-------------|
| `ecom_orders`        | Raw Fivetran-loaded table including ingestion metadata. |
| `bronze_ecom_orders` | Cleaned subset containing only business-relevant fields (order, customer, date, sales). |

---

### ğŸ¥ˆ Silver Layer â€“ Business Facts

| Table                   | Description |
|-------------------------|-------------|
| `silver_customer_facts` | Customer-level facts: first purchase, cohort month, second purchase, time-to-second-order. |
| `silver_order_facts`    | Order-level facts enriched with lifecycle tagging (new/existing) and monthly features. |

---

### ğŸ¥‡ Gold Layer â€“ Analytics Marts

| Table                     | Description |
|--------------------------|-------------|
| `gold_cohort_sizes`      | Customers per cohort month |
| `gold_cohort_retention`  | 1-, 2-, and 3-month retention |
| `gold_repeat_purchases`  | Share reaching 2+, 3+, 4+ orders |
| `gold_order_distribution`| Monthly orders by new vs. existing customers |

---

## ğŸ“ Databricks Unity Catalog Structure

All tables are stored inside Unity Catalog.

### âœ” Unity Catalog

![Databricks Catalog](img/databricks_catalog.png)

---

## 4. Pipeline Orchestration â€“ Databricks Jobs

The full pipeline runs as a Databricks job with:

- Bronze â†’ Silver â†’ Gold â†’ Dashboard dependency chain  
- Triggered by **Table Update** of `ecom_orders`  
- Serverless compute  
- Automated dashboard refresh
- Email notifications (on start, success, and failure â€“ including a snapshot of the refreshed cohort dashboard after a successful run)

### âœ” Successful DAG Run

![Databricks DAG](img/databricks_dag.png)

---

## 5. Serve Layer â€“ Cohort Analysis Dashboard

The final dashboard highlights key e-commerce KPIs:

### ğŸ“Š Metrics

- Monthly order volume (new vs. existing customers)
- Cohort sizes
- Retention after 1/2/3 months
- Repeat purchase depth (2+, 3+, 4+ orders)

### ğŸ” Insights (2024)

- **Customer acquisition declines sharply from February** and stops entirely after June.  
- **Early retention (1-month) peaks in Apr/May**, but declines overall, whereas **longer-term retention (2â€“3 months) remains stable**, indicating consistent return behavior across cohorts.
- **Repeat purchase depth weakens** â€” while most customers place a second order, fewer continue to a third or fourth purchase.

Overall â†’ **declining acquisition and weakening early repeat behavior indicate decreasing activation effectiveness across 2024**

### âœ” Dashboard Screenshot

![Dashboard](img/cohort_dashboard.png)

---

## 6. Repository Structure

```text
ETL-Project-Cohort-Analysis/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_bronze/
â”‚   â”œâ”€â”€ 02_silver/
â”‚   â”œâ”€â”€ 03_gold/
â”‚   â”œâ”€â”€ 04_views/
â”‚   â””â”€â”€ 90_inspect/
â”‚
â”œâ”€â”€ img/
â”‚   â”œâ”€â”€ source_bigquery.png
â”‚   â”œâ”€â”€ connector_fivetran.png
â”‚   â”œâ”€â”€ databricks_catalog.png
â”‚   â”œâ”€â”€ databricks_dag.png
â”‚   â””â”€â”€ cohort_dashboard.png
â”‚
â””â”€â”€ README.md
```

---

## 7. Technologies Used

- **Google BigQuery** â€“ cloud data warehouse used as the source system  
- **Fivetran** â€“ fully managed connector for automated, incremental data ingestion  
- **Databricks Lakehouse** â€“ Delta Lake storage, SQL & PySpark transformations, Jobs orchestration, Lakeview dashboards  
- **GitHub** â€“ version control and documentation

---

## 8. How to Reproduce

1. **Set up BigQuery**  
   Create a dataset and load the sample `ecom_orders` table.

2. **Configure Fivetran**  
   Connect BigQuery â†’ Databricks using the managed connector.

3. **Prepare Databricks**  
   Create a catalog & schema to store Bronze, Silver, and Gold tables.

4. **Connect Databricks to GitHub**  
   Create a project repo, link it as a Databricks Repo, and sync notebooks via Git.

5. **Run the transformation notebooks**  
   Execute Bronze â†’ Silver â†’ Gold notebooks to build the Medallion tables.

6. **Validate the tables**  
   Use the inspection notebook to preview Bronze/Silver/Gold outputs before orchestration.

7. **Create the Databricks Job**  
   Orchestrate Bronze â†’ Silver â†’ Gold using a table-update trigger.

8. **Build the Lakeview dashboard**  
   Visualize cohort sizes, retention, and repeat purchases.

---

## 9. Next Steps (Extension Options)

Given the limited sample dataset, meaningful improvements include:

- Add product, channel, or demographic dimensions  
- Introduce dbt for modular modeling & testing  
- Add data quality expectations  
- Publish dashboard externally (Tableau / Power BI)  
- Parameterize the pipeline for multiple datasets  

---

## ğŸ§‘â€ğŸ’» Author

**Thomas Jortzig**  
Cohort Retention Modeling & Customer Behavior Analysis â€“ Ecom Case Study (11/2025)
